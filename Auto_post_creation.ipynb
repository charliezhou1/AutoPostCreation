{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYxrJTdoK7Mmzvku7vCM0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charliezhou1/AutoPostCreation/blob/main/Auto_post_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto Post Creation with GPT-4 and LlamaIndex\n",
        "\n",
        "This project is to build an AI-powered content generator using **RAG (Retrieval-Augmented Generation)** with **OpenAI GPT-4**, **LangChain**, and **LlamaIndex**. It reads corporate reports from a directory, processes them with embeddings and LLMs, and outputs auto-generated content (e.g., LinkedIn posts or summaries).\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Features\n",
        "\n",
        "- Loads **corporate report documents** from Google Drive\n",
        "- Uses **OpenAI GPT-4** for post generation\n",
        "- Embeds documents using **text-embedding-3-small**\n",
        "- Built on **LlamaIndex** + **LangChain**\n",
        "- RAG-powered generation for relevant and grounded content\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WOQ368Dp6drj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDBiD1iTPGtf",
        "outputId": "3092b56a-5954-479d-d5b8-09e10ccabdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3135, in __init__\n",
            "    super().__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 124, in _parse_requirement_details\n",
            "    marker = _parse_requirement_marker(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 151, in _parse_requirement_marker\n",
            "    marker = _parse_marker(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n",
            "    expression = [_parse_marker_atom(tokenizer)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 291, in _parse_marker_atom\n",
            "    marker = _parse_marker_item(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 305, in _parse_marker_item\n",
            "    marker_var_right = _parse_marker_var(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 317, in _parse_marker_var\n",
            "    return process_python_str(tokenizer.read().text)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 332, in process_python_str\n",
            "    value = ast.literal_eval(python_str)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 964, in handle\n",
            "    rv = self.filter(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 820, in filter\n",
            "    if hasattr(f, 'filter'):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "*\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q\n",
        "\n",
        "\n",
        "import os\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"*\"\n",
        "print(os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Setup LLM model and embeddings\n",
        "Settings.llm = OpenAI(model='gpt-4-0125-preview', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive'  # Replace with your path\n",
        "os.makedirs(f'{data_dir}/RAG/data/corporate_reports/', exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moCRPim4PXoZ",
        "outputId": "367ef0c9-8716-4e73-9742-aeaa9a2f6560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "def load_reports_data():\n",
        "    reports_path = f'{data_dir}/RAG/data/corporate_reports/'\n",
        "    documents = SimpleDirectoryReader(reports_path).load_data()\n",
        "    return documents\n",
        "\n",
        "reports_documents = load_reports_data()"
      ],
      "metadata": {
        "id": "ZIg26N2mPu45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "\n",
        "PERSIST_INDEX_DIR = f'{data_dir}/RAG/storage/'\n",
        "\n",
        "def get_or_create_index(index_name, documents):\n",
        "    index_dir = f\"{PERSIST_INDEX_DIR}{index_name}/\"\n",
        "\n",
        "    # Ensure directory exists\n",
        "    if not os.path.exists(index_dir):\n",
        "        os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "    if not os.listdir(index_dir):  # Check if the directory is empty\n",
        "        print(f\"Creating index for {index_name}\")\n",
        "        # Create and persist the index\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        index.storage_context.persist(index_dir)\n",
        "    else:\n",
        "        print(f\"Loading existing index for {index_name}\")\n",
        "        # Load index from storage\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=index_dir)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "\n",
        "    return index\n",
        "\n",
        "corporate_index = get_or_create_index(\"corporate_reports\", reports_documents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9eouYC1QH_F",
        "outputId": "a60ee5b7-fbce-4cb9-867b-52bceb627c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing index for corporate_reports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBf8omaCc_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# Define a query engine for corporate reports\n",
        "corporate_engine = corporate_index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "# Create a tool for querying corporate reports\n",
        "corporate_query_tool = QueryEngineTool(\n",
        "    query_engine=corporate_engine,\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"Corporate_Reports\",\n",
        "        description=\"Generates summaries and insights from corporate reports for LinkedIn posts.\"\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq7Jl7klRN-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define post engines for different platforms\n",
        "def linkedin_post_engine(query):\n",
        "    return corporate_query_tool.query_engine.query(query)\n",
        "\n",
        "def reddit_post_engine(query):\n",
        "    # Customize summary for Reddit style\n",
        "    query = \"Generate a Reddit post summarizing the latest corporate report with a more casual tone.\"\n",
        "    return corporate_query_tool.query_engine.query(query)\n",
        "\n",
        "def twitter_post_engine(query):\n",
        "    # Customize summary for Twitter style (concise)\n",
        "    query = \"Generate a Twitter post summarizing the latest corporate report in a tweet-length format.\"\n",
        "    return corporate_query_tool.query_engine.query(query)"
      ],
      "metadata": {
        "id": "J22NDUZhTH5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Router Query Engine for selecting post engines\n",
        "def router_engine(query):\n",
        "    if \"LinkedIn\" in query:\n",
        "        return linkedin_post_engine(query)\n",
        "    elif \"Reddit\" in query:\n",
        "        return reddit_post_engine(query)\n",
        "    elif \"Twitter\" in query:\n",
        "        return twitter_post_engine(query)\n",
        "    else:\n",
        "        return \"No platform specified.\"\n",
        "\n",
        "# Example queries\n",
        "linkedin_query = \"Generate a LinkedIn post summarizing the key insights from the latest corporate report.\"\n",
        "reddit_query = \"Generate a Reddit post summarizing the key insights from the latest corporate report.\"\n",
        "twitter_query = \"Generate a Twitter post summarizing the key insights from the latest corporate report.\"\n",
        "\n",
        "# Example routing of queries\n",
        "linkedin_response = router_engine(linkedin_query)\n",
        "reddit_response = router_engine(reddit_query)\n",
        "twitter_response = router_engine(twitter_query)\n",
        "\n",
        "print(\"LinkedIn Response:\", linkedin_response)\n",
        "print(\"Reddit Response:\", reddit_response)\n",
        "print(\"Twitter Response:\", twitter_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRdGhQGGdNeu",
        "outputId": "589721c2-e5ff-49cd-d724-89e8aeeb90d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinkedIn Response: üöÄ Exciting Insights from Our Latest Corporate Report on AI! üöÄ\n",
            "\n",
            "We're thrilled to share groundbreaking findings from our recent survey on the state of AI in early 2024. With responses from 1,363 participants across various industries and regions, our research highlights the remarkable journey of AI and generative AI (gen AI) adoption and its burgeoning impact on businesses worldwide.\n",
            "\n",
            "üåê Key Highlights:\n",
            "- A significant surge in gen AI adoption, with 981 organizations implementing AI in at least one business function.\n",
            "- An impressive 878 organizations are now regularly using gen AI, showcasing its growing influence and utility in the business landscape.\n",
            "- Our analysis reveals that high-performing companies are leveraging highly customized or bespoke gen AI solutions to capture unparalleled business value, moving beyond off-the-shelf solutions to achieve competitive differentiation.\n",
            "\n",
            "üîç Deep Dive into Practices:\n",
            "- The survey sheds light on the practices of high performers, emphasizing the importance of embedding testing and validation in the model release process and developing clear processes for iterative model improvement.\n",
            "\n",
            "üìà The Future of AI:\n",
            "- As we navigate through the age of adoption to the age of value, our findings underscore the necessity for businesses to align people, processes, and technologies to unlock the full potential of AI. The journey towards harnessing the greatest value from AI involves a methodical approach, focusing on strategic use cases, leadership investment, and the implementation of MLOps processes.\n",
            "\n",
            "üåü Conclusion:\n",
            "- The landscape of AI is evolving rapidly, with gen AI at the forefront of this transformation. Businesses that adapt to these changes, embracing customization and strategic implementation, stand to gain significant competitive advantages.\n",
            "\n",
            "Stay tuned for more insights and discoveries as we continue to explore the evolving world of AI. Let's embark on this journey together, unlocking new opportunities and driving innovation in our industries!\n",
            "\n",
            "#AI #GenerativeAI #Innovation #BusinessTransformation #FutureOfWork\n",
            "Reddit Response: Hey folks! üöÄ Just dove into the latest scoop from McKinsey on the state of AI in early 2024, and let me tell you, it's quite the read. We're talking about a major spike in Gen AI adoption that's not just for show ‚Äì it's actually starting to churn out real value for businesses. \n",
            "\n",
            "So, McKinsey ran this massive survey, pulling in insights from over a thousand participants across the globe. The big headline? A whopping 981 organizations are now using AI in some form, and 878 of these are leveraging generative AI (yeah, the cool, creative AI) in at least one business function. \n",
            "\n",
            "But here's where it gets juicy: the real winners in the AI game are those crafting custom, tailored solutions. Think less off-the-shelf, more DIY on steroids. These companies aren't just using AI; they're molding it to fit their unique needs, whether that's by feeding it proprietary data or fine-tuning it for their specific industry. \n",
            "\n",
            "McKinsey's calling these innovators the \"shapers\" or \"makers,\" and they're the ones truly cashing in on AI's potential. Meanwhile, the \"takers\" ‚Äì those picking up generic AI solutions ‚Äì might see some benefits, but they're not hitting the jackpot like their customizing counterparts.\n",
            "\n",
            "The report also highlights the importance of embedding testing and validation in AI model releases, along with developing processes for iterative improvement. It's clear that as AI evolves, the game is shifting towards deep customization and continuous enhancement.\n",
            "\n",
            "In essence, if you're looking to not just participate in the AI revolution but to lead it, it's time to think beyond the plug-and-play solutions. The future belongs to the builders, the tinkerers, the customizers ‚Äì those ready to roll up their sleeves and tailor AI to their vision.\n",
            "\n",
            "And that's the lowdown from McKinsey's latest. For anyone looking to stay ahead in the AI curve, it's a must-read. Catch you on the flip side! üåê‚ú®\n",
            "Twitter Response: \"üöÄ The latest corporate report reveals a significant spike in Gen AI adoption, transforming businesses and generating unprecedented value as we move from the age of adoption to the age of value in 2024. #AI #Innovation #BusinessTransformation\"\n"
          ]
        }
      ]
    }
  ]
}